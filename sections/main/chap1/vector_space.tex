\subsection{Definition of Vector Space}
\begin{definition}[Vector Space]
	\label{def:vector_space}
	A \textbf{vector space} (over a field $\field$) consists of a set $V$ with two operations $``+"$ and $``\cdot"$ subject to the conditions that for all $\vec{v}, \vec{w}, \vec{u}\in V$ and scalars $r, s\in\field$:
	\begin{enumerate}
		\item \textbf{Closure under}:
		\begin{enumerate}[label=\roman*]
			\item Vector addition: $\vec{v} + \vec{w} \in V$.
			\item Scalar multiplication: $r\cdot\vec{v} \in V$.
		\end{enumerate}

		\item \textbf{Properties of vector addition}:
		\begin{enumerate}[label=\roman*]\addtocounter{enumii}{2}
			\item Commutativity: $\vec{v} + \vec{w} = \vec{w} + \vec{v}$.
			\item Associativity: $(\vec{v} + \vec{w}) + \vec{u} = \vec{v} + (\vec{w} + \vec{u})$.
		\end{enumerate} 

		\item \textbf{Properties of scalar multiplication}:
		\begin{enumerate}[label=\roman*]\addtocounter{enumii}{4}
			\item Associativity : $r\cdot(s\cdot\vec{v}) = (rs)\cdot \vec{v}$.
			\item Distributivity over scalar addition: $(r+s)\cdot\vec{v} = r\cdot\vec{v} + s\cdot\vec{v}$.	
			\item Distributivity over vector addition: $r\cdot(\vec{v} + \vec{w}) = r\cdot\vec{v} + r\cdot\vec{w}$.
		\end{enumerate} 

		\item \textbf{Inverse elements}:
		\begin{enumerate}[label=\roman*]\addtocounter{enumii}{7}
			\item Additive inverse: $\forall \vec{v}\in V, \exists -\vec{v}\in V: \vec{v} + (-\vec{v}) = \vec{0}$.	
		\end{enumerate} 

		\item \textbf{Identity elements}:
		\begin{enumerate}[label=\roman*]\addtocounter{enumii}{8}
			\item Additive identity: $\exists\vec{0} \in V: \vec{0} + \vec{v} = \vec{v}, \quad \forall \vec{v}\in V$.
			\item Multiplicative identity: $\exists \bm{1}\in\field: \bm{1}\cdot\vec{v} = \vec{v},\quad \forall \vec{v} \in V$.	
		\end{enumerate} 
	\end{enumerate} 

	\noindent For brevity, we will denote vectors as bold face letters instead of overhead arrows after this definition. For example, $\vu, \vv$ and $\vw$.
\end{definition} 

\begin{remark}[``Over a field'']
	When we use the phrase ``a vector space over a field $\field$'', this means that the scalars that we use will be taken from the field $\field$. It does not mean that our vector space consists of $\field$-valued vectors. For example, the following vector space:
	\begin{align*}
		\mathrm{L} = \bigCurl{(x, \alpha x) : x\in\mathbb{C}, \alpha\in\R}	
	\end{align*}
	is a vector space over $\R$ (scalar multiplications are done with real-valued scalars) even though the vectors are complex-valued.
\end{remark} 

\begin{remark}[Trivial Space]
	A vector space with one element is called a \textbf{trivial space}.	
\end{remark} 

\begin{example}[A simple example]
	\label{example:simple_vector_space}
	The following is a vector space over $\R$:
	\begin{align*}
		\mathrm{L} = \bigCurl{
			\begin{pmatrix}
				x & y
			\end{pmatrix}^\top: y = 3x 
		}.
	\end{align*}

	\noindent This is easy to verify. Let us go through each condition one by one. Let the vectors $\vu_1, \vu_2, \vu_3 \in \mathrm{L}$ defined as follows: 
	\begin{align*}
		\vu_1 = \begin{pmatrix}
			x_1 \\ 3x_1	
		\end{pmatrix}, \quad
		\vu_2 = \begin{pmatrix}
			x_2 \\ 3x_2	
		\end{pmatrix}, \quad 
		\vu_3 = \begin{pmatrix}
			x_3 \\ 3x_3	
		\end{pmatrix}..
	\end{align*} 

	\noindent All the axioms of a vector space are satisfied. Let $\alpha,\beta\in\R$, we have:
	\begin{enumerate}
		\item \emph{Closure under vector addition}: $\vu_1 + \vu_2 = \begin{pmatrix} x_1 \\ 3x_1 \end{pmatrix} + \begin{pmatrix} x_2 \\ 3x_2 \end{pmatrix} = \begin{pmatrix} x_1 + x_2 \\ 3(x_1 + x_2) \end{pmatrix} \in \mathrm{L}$.

		\item \emph{Closure under scalar multiplication}: $\alpha\vu_1 = \alpha \begin{pmatrix} x_1 \\ 3x_1 \end{pmatrix} = \begin{pmatrix} \alpha x_1 \\ 3\alpha x_1 \end{pmatrix} \in \mathrm{L}$. 

		\item \emph{Additive commutativity}: $\vu_1 + \vu_2 = \begin{pmatrix} x_1 + x_2 \\ 3x_1 + 3x_2 \end{pmatrix} = \begin{pmatrix} x_2 + x_1 \\ 3x_2 + 3x_1 \end{pmatrix} = \vu_2 + \vu_1$.

		\item \emph{Additive associativity}: $(\vu_1 + \vu_2) + \vu_3 = \begin{pmatrix} (x_1 + x_2) + x_3 \\ 3(x_1 + x_2) + 3x_3\end{pmatrix}=\begin{pmatrix} x_1 + (x_2 + x_3) \\ 3x_1 + 3(x_2 + x_3)\end{pmatrix} = \vu_1 + (\vu_2 + \vu_3)$.

		\item $\dots$ (We can easily verify other axioms as well).
	\end{enumerate} 
\end{example} 

\begin{example}[Polynomials of degree 3]
	Consider the following set of real-coefficients polynomials with degree of at most 3:
	\begin{align*}
		\mathcal{P}_3 &= \bigCurl{
			a_0 + a_1x + a_2x^2 + a_3x^3 \Big| a_0, a_1, a_2, a_3\in\R
		}.
	\end{align*} 

	\noindent Then, $\mathcal{P}_3$ is a vector space over $\R$ under the following operations:
	\begin{align*}
		(a_0 + a_1x + a_2x^2 + a_3x^3) &+ (b_0 + b_1x + b_2x^2 + b_3x^3) \\
			&= (a_0 + b_0) + (a_1 + b_1)x + (a_2 + b_2)x^2 + (a_3 + b_3)x^3, \\
		\alpha\cdot(a_0 + a_1x + a_2x^2 + a_3x^3) &= (\alpha a_0) + (\alpha a_1) x + (\alpha a_2) x^2 + (\alpha a_3) x^3.
	\end{align*} 

	\noindent We can think of $\mathcal{P}_3$ as being ``the same'' as the vector space $\R^4$. For every set of real coefficients $a_0, \dots, a_3$, we have the following correspondence:
	\begin{align*}
		a_0 + a_1x + a_2x^2 + a_3x^3 \text{ corresponds to } \begin{pmatrix}
			a_0 \\ a_1 \\ a_2 \\ a_3	
		\end{pmatrix}.
	\end{align*}

	\noindent Similarly, for any $n\ge 1$, $\mathcal{P}_n$ is also a vector space over $\R$.
\end{example} 

\begin{definition}[Vector Space as Abelian Group]
	Let $V$ be a vector space over a field $\field$ with two operations $``+"$ and $``\cdot"$. Then, $(V, +)$ is an (additive) Abelian group that satisfies the additional axioms on scalar multiplications:
	\begin{enumerate}[label=\roman*]
		\item $\lambda\vu \in V$ for all $\lambda\in\field$ and $\vu\in V$. \hfill (Closure under scalar multiplication)
		\item $(\lambda\mu)\vu = \lambda(\mu\vu)$ for all $\lambda, \mu\in\field$ and $\vv\in V$. \hfill (Associativity)
		\item $\lambda(\vu + \vv) = \lambda\vu + \lambda\vv$, $\vu, \vv\in V$ and $\lambda\in\field$. \hfill (Distributivity in $V$)
		\item $(\lambda + \mu)\vu = \lambda\vu + \mu\vu$, $\lambda, \mu\in\field$ and $\vu\in V$. \hfill (Distributivity in $\field$)
		\item $1\vu = \vu$ for all $\vu\in V$. \hfill (Multiplicative identity)
	\end{itemize} 

	\noindent Basically, we can think of vector spaces as Abelian groups with additional structures involving scalar multiplications. According to definition \ref{def:vector_space}, the fact that $V$ is an Abelian group makes it satisfy properties $(i), (iii), (iv), (vii)$ and $(viii)$. The additonal properties regarding scalar multiplication are listed above.
\end{definition} 

\subsection{Subspace and Spanning Sets}
In example \ref{example:simple_vector_space}, we saw a vector space over $\R$ that is a subset of $\R^2$, $\mathrm{L}=\{(x, 3x)^\top : x\in\R\}$, which is a line through the origin. In this section, we provide a formal definition for subspace of a vector space:
\begin{definition}[Subspace]
	For vector space $V$ with two operations $``+"$ and $``\cdot"$. $S$ is called a subspace of $V$ if:
	\begin{itemize}
		\item $S\subseteq V$.
		\item $S$ is a vector space under the inherited operations.	
	\end{itemize} 	
\end{definition} 

\begin{proposition}{Closure of Subspace under Linear Combination}{closure_of_subspace}
	For a non-empty subset $S$ of a vector space $V$ over a field $\field$. Under the inherited operations, the following statements are equivalent:
	\begin{enumerate}
		\item $S$ is a subspace of $V$.
		\item $S$ is closed under linear combinations of any number of vectors: For all $\lambda_1, \dots, \lambda_n\in\field$ and $\vu_1, \dots, \vu_n\in S$, $\sum_{i=1}^n\lambda_i\vu_i\in S$.	
	\end{enumerate} 
\end{proposition} 

\noindent The above proposition gives us a convenient tool to work with whenever we want to prove that some subset of a vector space is a subspace. Instead of going through all the axioms in definition \ref{def:vector_space}, we can just prove that the subset is closed under finite linear combinations. For example, we have the following proposition:


\begin{proposition}{Operations that Preserves Subspaces}{operations_that_preserve_subspaces}
	Let $V$ be a vector space over a field $\field$ and $U, W \subseteq V$ be subspaces. Then, the following subsets of $V$:
	\begin{enumerate}
		\item $U + W = \{\vu + \vw: \vu\in U, \vw \in W\}$\footnote{Later we will learn that the direct sum $U\oplus W$ is also a subspace. Basically $U\oplus W=U+W$ where $U\cap W=\{0\}$.},
		\item $U \cap W = \{\vv \in V: \vv \in U \text{ and } \vv\in W\}$
	\end{enumerate} 

	\noindent are all subspaces of $V$.
\end{proposition} 

\begin{proof*}[Proposition \ref{prop:operations_that_preserve_subspaces}]
	We know that $U+W$ is definitely a subset of $V$ because of its closure under vector addition. It is also trivial that $U\cap W\subseteq V$. Now we prove that any finite linear combination of each subset belongs to itself.
	\begin{enumerate}
		\item $U+W$: Let $\lambda_1, \dots, \lambda_n \in \field$ be a finite sequence of scalars and $\vv_1, \dots, \vv_n\in U+W$. Then, for any $1\le i \le n$, we have $\vv_i = \vu_i + \vw_i$ where $\vu_i\in U$ and $\vw_i \in W$. Therefore, we have:
		\begin{align*}
			\sum_{i=1}^n \lambda_i \vv_i &= \sum_{i=1}^n \lambda_i \vu_i + \sum_{i=1}^n \lambda_i \vw_i.
		\end{align*} 

		\noindent Since we have $\sum_{i=1}^n \lambda_i \vu_i\in U$ and $\sum_{i=1}^n \lambda_i \vw_i \in W$ due to closure under linear combinations of subspaces, we have $\sum_{i=1}^n \lambda_i \vv_i \in U+W$. Therefore, $U+W$ is also closed under linear combinations and hence, a subspace of $V$.

		\item $U\cap W$: Let $\vv_1, \dots, \vv_n \in U\cap W$ and $\lambda_1, \dots, \lambda_n\in\field$. It holds that $\sum_{i=1}^n\lambda_i\vv_i \in U$ because $U$ is a subspace. Since $W$ is also a subspace $\sum_{i=1}^n \lambda_i\vv_i \in W$. Therefore, $\sum_{i=1}^n\lambda_i\vv_i\in U\cap W$. 
	\end{enumerate} 
\end{proof*} 

\begin{definition}[Span (Linear Closure)]
	The span of a non empty subset $S$ of a vector space over a field $\field$ is the set of all linear combinations of vectors from $S$.
	\begin{equation}
		\mathrm{span}(S) = \biggCurl{
			\sum_{i=1}^n \lambda_i \vv_i : \vv_i \in S, \lambda_i \in \field
		}.
	\end{equation} 

	\noindent The span of an empty subset of a vector space is its trivial space ($\mathrm{span}(\emptyset) = \{0\}$).
\end{definition} 

\begin{proposition}{Span of Subsets in Vector Spaces}{span_of_subsets_in_vspaces}
	In a vector space, the span of any subset is a subspace.
\end{proposition} 

\begin{proof*}[Proposition \ref{prop:span_of_subsets_in_vspaces}]
	Let $V$ be a vector space over $\field$ and $S\subseteq V$. Denote that $\tilde S = \mathrm{span}(S)$. Obviously $\tilde S \subseteq V$ due to closure under linear combinations of vector spaces. We have to prove that $\tilde S$ is also closed under linear combinations. Let $\lambda_1, \dots, \lambda_n\in\field$ and $\vv_1, \dots, \vv_n\in \tilde S$. For each $\vv_i$, we have:
	\begin{align*}
		\vv_i = \sum_{j=1}^{n_i} \alpha_{i, j} \vu_{i,j} \text{ where } \alpha_{i,1}, \dots, \alpha_{i, n_i} \in \field, \vu_{i,1}, \dots, \vu_{i,n_i} \in S.
	\end{align*} 

	\noindent Therefore, we have:
	\begin{align*}
		\sum_{i=1}^n \lambda_i \vv_i &= \sum_{i=1}^n \sum_{j=1}^{n_i} \alpha_{i, j}\vu_{i, j} \in \tilde S.
	\end{align*} 

	\noindent Therefore, $\tilde S$ is a subspace of $V$.
\end{proof*}	 